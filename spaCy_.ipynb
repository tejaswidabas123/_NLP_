{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OX_x6GqV11U"
   },
   "source": [
    "# spaCy  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y3LpcE_V11V"
   },
   "source": [
    "Overview:  \n",
    "-spaCy Install (cuda support)      \n",
    "-English language Install  \n",
    "-Tokenization and Parts-of-Speech Tagging     \n",
    "-Using word vectors and Similarity   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CiGOUiOV11W"
   },
   "source": [
    "# spaCy is another popular NLP library\n",
    "\n",
    "spaCy is a new Python NLP library with great features: focuses on production usage and supports deep learning frameworks. Excellent for very large text files.  (NLTK is more of a starter tool for language processing.)  \n",
    "\n",
    "Capabilities:    \n",
    "-Non-destructive tokenization  \n",
    "-Named entity recognition  \n",
    "-Support for 55+ languages  \n",
    "-17 statistical models for 11 languages  \n",
    "-pretrained word vectors  \n",
    "-State-of-the-art speed  \n",
    "-Easy deep learning integration  \n",
    "-Part-of-speech tagging  \n",
    "-Labeled dependency parsing  \n",
    "-Syntax-driven sentence segmentation  \n",
    "-Built in visualizers for syntax and NER  \n",
    "-Convenient string-to-hash mapping  \n",
    "-Export to numpy data arrays  \n",
    "-Efficient binary serialization  \n",
    "-Easy model packaging and deployment  \n",
    "-Robust, rigorously evaluated accuracy  \n",
    "   \n",
    "https://spacy.io/docs/#examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7S8-SPCV11X"
   },
   "source": [
    "**Excellent article on comparison of NLTK libraries:**    \n",
    "https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EXtBBfwV11X"
   },
   "source": [
    "**cuda support for spaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80ExYeo_V11X"
   },
   "source": [
    "**spaCy v2.0** comes with neural network models that are implemented in the machine learning library Thinc. For GPU support, we’ve been grateful to use the work of Chainer’s CuPy module which provides a numpy-compatible interface for GPU arrays.    \n",
    "**spaCy** can be installed on GPU by specifying:  \n",
    "`pip install spacy[cuda]\n",
    "pip install spaCy[cuda112]`\n",
    "\n",
    "Or any other cuda version you have: spacy[cuda90], spacy[cuda91], spacy[cuda92] or spacy[cuda100]       \n",
    "If you know your cuda version, using the more explicit specifier allows `cupy` to be installed via wheel saving some compilation time. The specifiers should install `cupy`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7OeUrOXV11X"
   },
   "source": [
    "If you have issues with **spaCy for CUDA (GPU)**, uninstall it\n",
    "#!pip uninstall spacy[cuda90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYtgJKx9V11Y"
   },
   "source": [
    "To activate gpu use:\n",
    "#spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4WFYKdySV11Y"
   },
   "outputs": [],
   "source": [
    "# To install reference:  https://spacy.io/usage\n",
    "# pip install -U spacy\n",
    "#pip install --upgrade spacy  #alternatively\n",
    "#\n",
    "# or uncomment the following and install in the python notebook\n",
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m6zshVOUV11Z"
   },
   "outputs": [],
   "source": [
    "#If you have not updated python pip command run the following command:\n",
    "#!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVCcsAacV11a"
   },
   "source": [
    "Some commands such as the following may require administrator priviledges or you will receive a warning: You do not have sufficient privilege to perform this operation.      \n",
    "Open a cmd window as an Administrator, run the command and then open a Jupyter notebook.  \n",
    "On Linux all installations whould be done as sudo user  \n",
    "#!python -m spacy download en  \n",
    "#!sudo python -m spcy download en  # Linux  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J6tYj3PV11a"
   },
   "source": [
    "# Install Language Modules\n",
    "https://spacy.io/models/en  \n",
    "Need to also install language modules separately. They come in sizes, small (sm), medium (md), and large (lg).  \n",
    "To install large English module:  \n",
    "    \n",
    "    $ python -m spacy download en_core_web_lg\n",
    "    \n",
    "Spacy has 4 English modules:    \n",
    "* en_core_web_sm (assigns context-specific token vectors, POS tags, dependency parse and named entities. Contains no vectors.)  \n",
    "* en_core_web_md  (assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.)    \n",
    "* en_core_web_lg  (same as above)    \n",
    "* en_vectors_web_lg (same as above, Contains > 1 million unique vectors.)  \n",
    "\n",
    "**Important: You download modules in the command window as Administrator (at least for Windows.)**\n",
    "Once you run the above command, you will be informed:  \n",
    "You can now load the model via spacy.load('en')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDwt-IfuV11a",
    "outputId": "4b1c61d4-c9e9-48fc-c6af-561940b8c426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 22:06:19.219004: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-13 22:06:19.219071: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-13 22:06:19.219114: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-13 22:06:19.231119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 22:06:21.424241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-11-13 22:06:25.471246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 22:06:25.471884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 22:06:25.472120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "Collecting en-core-web-lg==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "#python -m spacy download en\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XW8ihHRiV11a"
   },
   "outputs": [],
   "source": [
    "#Make sure one of the languages is downloaded e.g., python -m spacy download en_core_web_lg\n",
    "import spacy\n",
    "#parser = spacy.load('en') #specifies the language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEY-fLZ4V11a"
   },
   "source": [
    "# Tokenization in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yu39qEiV11a"
   },
   "source": [
    "Tokenization is the process of breaking up a sequences of texts into words, keywords, phrases, symbols, or sentences called tokens.   \n",
    "Each individual part is known as a **token**.\n",
    "Typically punctuation is removed.\n",
    "The input to the tokenizer is a unicode text, and the output is a Doc object.\n",
    "To construct a Doc object, you need a Vocab instance, a sequence of word strings, and optionally a sequence of spaces booleans, which allow you to maintain alignment of the tokens into the original string.\n",
    "\n",
    "Important note spaCy’s tokenization is non-destructive, which means that you’ll always be able to reconstruct the original input from the tokenized output. Whitespace information is preserved in the tokens and no information is added or removed during tokenization. This is kind of a core principle of spaCy’s Doc object: doc.text == input_text should always hold true.\n",
    "\n",
    "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token. Each Doc consists of individual tokens, and we can iterate over them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1wvVR0FV11b"
   },
   "source": [
    "**Tokenizer exception**   \n",
    "Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied.\n",
    "Prefix: Character(s) at the beginning, e.g. $, (, “, ¿.\n",
    "Suffix: Character(s) at the end, e.g. km, ), ”, !.\n",
    "Infix: Character(s) in between, e.g. -, --, /, ….\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n",
    "If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HzMJAeTV11b",
    "outputId": "a14aa967-fb7b-4d87-8cef-7c1a71c6cffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We hold these truths to be self-evident, that all men are created equal, that they are endowed, by their Creator, with certain unalienable Rights, that among these are Life, Liberty, and the pursuit of Happiness.  That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or abolish it, and to institute new Government, laying its foundation on such principles, and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.\n"
     ]
    }
   ],
   "source": [
    "#Preamble to the U.S. Declaration of Independence, 1776\n",
    "preamble='We hold these truths to be self-evident, that all men are created equal, that they are endowed, by their Creator, with certain unalienable Rights, that among these are Life, Liberty, and the pursuit of Happiness.  That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or abolish it, and to institute new Government, laying its foundation on such principles, and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.'\n",
    "print(preamble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAQ1OqGKV11b",
    "outputId": "293c0b6f-2e70-46ef-856d-060b68f53ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'hold', 'these', 'truths', 'to', 'be', 'self', '-', 'evident', ',', 'that', 'all', 'men', 'are', 'created', 'equal', ',', 'that', 'they', 'are', 'endowed', ',', 'by', 'their', 'Creator', ',', 'with', 'certain', 'unalienable', 'Rights', ',', 'that', 'among', 'these', 'are', 'Life', ',', 'Liberty', ',', 'and', 'the', 'pursuit', 'of', 'Happiness', '.', 'That', 'to', 'secure', 'these', 'rights', ',', 'Governments', 'are', 'instituted', 'among', 'Men', ',', 'deriving', 'their', 'just', 'powers', 'from', 'the', 'consent', 'of', 'the', 'governed', ',', 'That', 'whenever', 'any', 'Form', 'of', 'Government', 'becomes', 'destructive', 'of', 'these', 'ends', ',', 'it', 'is', 'the', 'Right', 'of', 'the', 'People', 'to', 'alter', 'or', 'abolish', 'it', ',', 'and', 'to', 'institute', 'new', 'Government', ',', 'laying', 'its', 'foundation', 'on', 'such', 'principles', ',', 'and', 'organizing', 'its', 'powers', 'in', 'such', 'form', ',', 'as', 'to', 'them', 'shall', 'seem', 'most', 'likely', 'to', 'effect', 'their', 'Safety', 'and', 'Happiness', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokens = parser(preamble) #Alternatively to read data into a token object\n",
    "tokens = nlp(preamble) #Alternatively to read data into a token object\n",
    "\n",
    "# methods: rth_ checks verbatim text content, isspace() checks if string is a space\n",
    "token_words = [token.orth_ for token in tokens if not token.orth_.isspace()]\n",
    "print(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykEBky0CV11c",
    "outputId": "a397ca44-68df-4a29-a575-8faad701a08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We hold these truths to be self-evident, that all men are created equal, that they are endowed, by their Creator, with certain unalienable Rights, that among these are Life, Liberty, and the pursuit of Happiness.  That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or abolish it, and to institute new Government, laying its foundation on such principles, and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.\n"
     ]
    }
   ],
   "source": [
    "#Alternatively to read data into a token object\n",
    "tokens = nlp(preamble)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTf91tJmV11c"
   },
   "source": [
    "# Merging and splitting tokens  \n",
    "The `Doc.retokenize` context manager lets you merge and split tokens. Modifications to the tokenization are stored and performed all at once when the context manager exits. To merge several tokens into one single token, pass a Span to retokenizer.merge. An optional dictionary of attrs lets you set attributes that will be assigned to the merged token – for example, the lemma, part-of-speech tag or entity type. By default, the merged token will receive the same attributes as the merged span’s root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xy6WNC-pV11c",
    "outputId": "f10585ab-d441-4bf4-da4e-48f05ca73c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['I', 'live', 'in', 'New', 'York']\n",
      "After: ['I', 'live', 'in', 'New York']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I live in New York\")\n",
    "print(\"Before:\", [token.text for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new york\"})\n",
    "print(\"After:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfxsNsWnV11c"
   },
   "source": [
    "# Part of Speech Tagging\n",
    "After tokenization, spaCy can parse and tag a given document object. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in the context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalizes across the language – for example, a word following “the” in English is most likely a noun.\n",
    "\n",
    "Linguistic annotations are available as Token attributes. **Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency**. To obtain the readable string representation of an attribute we need to add an underscore _ to its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cmjmg7ybV11c",
    "outputId": "59c074c8-2a18-4eda-c265-9e0942e4266e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['We', 'these truths', 'all men', 'they', 'their Creator', 'certain unalienable Rights', 'these', 'Life', 'Liberty', 'the pursuit', 'Happiness', 'That', 'these rights', 'Governments', 'Men', 'their just powers', 'the consent', 'any Form', 'Government', 'these ends', 'it', 'the Right', 'the People', 'it', 'new Government', 'its foundation', 'such principles', 'its powers', 'such form', 'them', 'their Safety', 'Happiness']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\",[chunk.text for chunk in tokens.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWu98rZbV11d",
    "outputId": "79bbe149-cc26-462d-efea-3badbc01555e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb phrases: ['hold', 'create', 'endow', 'secure', 'institute', 'derive', 'govern', 'become', 'alter', 'abolish', 'institute', 'lay', 'organize', 'seem', 'effect']\n"
     ]
    }
   ],
   "source": [
    "print(\"Verb phrases:\",[token.lemma_ for token in tokens if token.pos_ ==\"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPw6ikfzV11d",
    "outputId": "7b5dcb5e-014e-4f1d-8bd3-abc3049b7562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rights ORG\n",
      "Happiness ORG\n",
      "Men ORG\n",
      "the Right of the People WORK_OF_ART\n",
      "institute new Government ORG\n",
      "Safety and Happiness ORG\n"
     ]
    }
   ],
   "source": [
    "#Find named entities, phrases and concepts\n",
    "for entity in tokens.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWu312baV11d",
    "outputId": "696015dd-66bd-4d3d-af4c-88b0d878c7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "a a DET DT det x True True\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"Apple is looking at buying a U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xsna9hTV11d",
    "outputId": "5303bfaf-0826-4a2c-bb65-8c6e485480ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Smith', 'loves', 'tacos', '.', 'He', 'has', 'a', 'Ph.D.', 'in', 'taco', '-', 'ology', '.']\n"
     ]
    }
   ],
   "source": [
    "#spaCy does not make a sentence segmentation error and treats Ph.D. correctly just like NLTK\n",
    "text2 = \"Mr. Smith loves tacos. He has a Ph.D. in taco-ology.\"\n",
    "tokens = nlp(text2)\n",
    "tokens = [token.orth_ for token in tokens if not token.orth_.isspace()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wHn0DHjV11d"
   },
   "source": [
    "A comparison to **nltk**.  Notice how the - in taco-ology separates words in **spaCy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDX1PRomWefW",
    "outputId": "ccc2899c-a3ee-4b28-be30-61f1f0b3b9d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEK6I7GaV11d",
    "outputId": "035077af-163d-4416-ca7a-379bb7b26710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Smith', 'loves', 'tacos', '.', 'He', 'has', 'a', 'Ph.D.', 'in', 'taco-ology', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text2 = \"Mr. Smith loves tacos. He has a Ph.D. in taco-ology.\"\n",
    "tokens = nltk.word_tokenize(text2)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV9LEqNhV11d"
   },
   "source": [
    "<p>In earlier NL days of spaCy.en (English language) exceptions were added like:    \n",
    "\n",
    "<p>spacy.en.English.Defaults.tokenizer_exceptions[\"Ph.D.\"] = [{\"F\": \"Ph.D.\"}]  \n",
    "\n",
    "<p>What you're doing here is telling the tokenizer, \"when you see this chunk, 'Ph.D.', process it into these tokens\".\n",
    "<p>The list you're giving spaCy has just one token, and you've specified its form (with the F key).\n",
    "<p>You can also specify its POS and lemma (with P and L)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wDA_QGgV11e",
    "outputId": "ac8cb376-1f68-47f9-cc13-bb7cb06dedf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'capitalize',\n",
       " 'casefold',\n",
       " 'center',\n",
       " 'count',\n",
       " 'encode',\n",
       " 'endswith',\n",
       " 'expandtabs',\n",
       " 'find',\n",
       " 'format',\n",
       " 'format_map',\n",
       " 'index',\n",
       " 'isalnum',\n",
       " 'isalpha',\n",
       " 'isascii',\n",
       " 'isdecimal',\n",
       " 'isdigit',\n",
       " 'isidentifier',\n",
       " 'islower',\n",
       " 'isnumeric',\n",
       " 'isprintable',\n",
       " 'isspace',\n",
       " 'istitle',\n",
       " 'isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'maketrans',\n",
       " 'partition',\n",
       " 'removeprefix',\n",
       " 'removesuffix',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following methods can be used on tokens.\n",
    "dir(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IEFH4lLeV11e"
   },
   "outputs": [],
   "source": [
    "# Define a function that prints the most essential attributes of a token.\n",
    "def print_token(tokens):\n",
    "    print(\"==========================\")\n",
    "    print(\"value:\",token.text)   #  Verbatim text\n",
    "    print(\"lemma:\",token.lemma_) # lemma is the root of a word\n",
    "    print(\"shape:\",token.shape_) # shape is capitalization and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqYwfhr5V11e",
    "outputId": "b9702e97-9138-4029-8633-5b40a1f75ce0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "value: Jack\n",
      "lemma: Jack\n",
      "shape: Xxxx\n",
      "==========================\n",
      "value: flew\n",
      "lemma: fly\n",
      "shape: xxxx\n",
      "==========================\n",
      "value: to\n",
      "lemma: to\n",
      "shape: xx\n",
      "==========================\n",
      "value: the\n",
      "lemma: the\n",
      "shape: xxx\n",
      "==========================\n",
      "value: store\n",
      "lemma: store\n",
      "shape: xxxx\n",
      "==========================\n",
      "value: because\n",
      "lemma: because\n",
      "shape: xxxx\n",
      "==========================\n",
      "value: he\n",
      "lemma: he\n",
      "shape: xx\n",
      "==========================\n",
      "value: was\n",
      "lemma: be\n",
      "shape: xxx\n",
      "==========================\n",
      "value: hungry\n",
      "lemma: hungry\n",
      "shape: xxxx\n",
      "==========================\n",
      "value: and\n",
      "lemma: and\n",
      "shape: xxx\n",
      "==========================\n",
      "value: forgot\n",
      "lemma: forgot\n",
      "shape: xxxx\n",
      "==========================\n",
      "value: milk\n",
      "lemma: milk\n",
      "shape: xxxx\n",
      "==========================\n",
      "value: and\n",
      "lemma: and\n",
      "shape: xxx\n",
      "==========================\n",
      "value: eggs\n",
      "lemma: egg\n",
      "shape: xxxx\n",
      "==========================\n",
      "value: .\n",
      "lemma: .\n",
      "shape: .\n"
     ]
    }
   ],
   "source": [
    "# Note the lemma for \"was/be\" and \"flew/fly\"\n",
    "text3 = \"Jack flew to the store because he was hungry and forgot milk and eggs.\"\n",
    "tokens = nlp(text3)\n",
    "for token in tokens:\n",
    "    print_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V33R7Y-PV11e",
    "outputId": "03240242-0e80-4c9c-c987-4cfd04ac316e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 22:08:39.996814: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-13 22:08:39.996879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-13 22:08:39.996916: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-13 22:08:41.172239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Collecting de-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# Alternative, load the German language\n",
    "!python -m spacy download de_core_news_sm\n",
    "import de_core_news_sm\n",
    "nlp2 = de_core_news_sm.load()   # notice that we using module_name.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9KVzw88V11e",
    "outputId": "fb1a17dc-7a34-47de-e357-9edae801ac17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. “I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week.\n"
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Zs8boI0V11e",
    "outputId": "c8a2c670-4749-4a37-c82b-4fc05a6e016e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "******************\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "******************\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print('******************')\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "print('******************')\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxByGTf1V11e",
    "outputId": "1e428001-4d68-41d3-efe4-cd4d59e864d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dep xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeDGy9wkV11e"
   },
   "source": [
    "Text: The original word text.   \n",
    "Lemma: The base form of the word.   \n",
    "POS: The simple part-of-speech tag.   \n",
    "Tag: The detailed part-of-speech tag.\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.   \n",
    "Shape: The word shape – capitalization, punctuation, digits.\n",
    "isalpha: Is the token an alpha character?   \n",
    "isstop: Is the token part of a stop list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI7ku9KUV11f"
   },
   "source": [
    "To understand entities:   \n",
    "Use:  \n",
    "**spacy.explain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LW26bUyDV11f",
    "outputId": "20762d47-fa1c-444c-8a57-73d8c1483790"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'verb, 3rd person singular present'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"VBZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WUqUNyiV11f"
   },
   "source": [
    "# Visualizer\n",
    "\n",
    "Visualizing a dependency parse or named entities in a text is not only a fun NLP demo – it can also be incredibly helpful in speeding up development and debugging your code and training proces\n",
    "Using spaCy’s built-in displaCy visualizer, here’s what our example sentence and its dependencies look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cg3CwyEeV11f",
    "outputId": "430adbdc-e918-415f-fd76-baa366289c54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'spacy.displacy' from '/usr/local/lib/python3.10/dist-packages/spacy/displacy/__init__.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6TVtQoHV11f",
    "outputId": "0ffcd598-ca15-4a42-8e7b-fe9e7c860c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "### this cell migth stall your notebook\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "displacy.serve(doc, style=\"dep\")  #ent or dep\n",
    "\n",
    "\n",
    "# https://spacy.io/usage/visualizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQNb-WTxV11f"
   },
   "source": [
    "# Dependency Parsing\n",
    "spaCy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree. The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or “chunks”. You can check whether a Doc object has been parsed with the doc.is_parsed attribute, which returns a boolean value. If this attribute is False, the default sentence iterator will raise an exception.\n",
    "\n",
    "# Noun chunks\n",
    "Noun chunks are “base noun phrases” – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, “the lavish green grass” or “the world’s largest tech fund”. To get the noun chunks in a document, simply iterate over Doc.noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yx7c0UNkV11f",
    "outputId": "8bb572b9-8518-4321-89b5-dec0b4d22283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars - cars - nsubj shift\n",
      "insurance liability - liability - dobj shift\n",
      "manufacturers - manufacturers - pobj toward\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text,'-', chunk.root.text, '-', chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_nkaWHqV11g"
   },
   "source": [
    "Text: The original noun chunk text.  \n",
    "Root text: The original text of the word connecting the noun chunk to the rest of the parse.   \n",
    "Root dep: Dependency relation connecting the root to its head.  \n",
    "Root head text: The text of the root token’s head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-8_U5TwV11g"
   },
   "source": [
    "`TEXT\tROOT.TEXT\tROOT.DEP_\tROOT.HEAD.TEXT\n",
    "Autonomous cars\tcars\tnsubj\tshift\n",
    "insurance liability\tliability\tdobj\tshift\n",
    "manufacturers\tmanufacturers\tpobj\ttoward`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps5447jJV11g"
   },
   "source": [
    "Navigating the parse tree\n",
    "spaCy uses the terms head and child to describe the words connected by a single arc in the dependency tree. The term dep is used for the arc label, which describes the type of syntactic relation that connects the child to the head. As with other attributes, the value of .dep is a hash value. You can get the string value with .dep_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hY0NYK6V11g",
    "outputId": "5b372090-c12c-4e63-e695-d85011f3de5e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous amod cars NOUN []\n",
      "cars nsubj shift VERB [Autonomous]\n",
      "shift ROOT shift VERB [cars, liability, toward]\n",
      "insurance compound liability NOUN []\n",
      "liability dobj shift VERB [insurance]\n",
      "toward prep shift VERB [manufacturers]\n",
      "manufacturers pobj toward ADP []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFfhx9pTV11g"
   },
   "source": [
    "`Text: The original token text.   \n",
    "Dep: The syntactic relation connecting child to head.   \n",
    "Head text: The original text of the token head.   \n",
    "Head POS: The part-of-speech tag of the token head.   \n",
    "Children: The immediate syntactic dependents of the token.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKdEQSGOV11g"
   },
   "source": [
    "Because the syntactic relations form a tree, every word has exactly one head. You can therefore iterate over the arcs in the tree by iterating over the words in the sentence. This is usually the best way to match an arc of interest — from below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Op3uvNGTV11g",
    "outputId": "600bee88-dd3f-4fa7-c0d6-4a09c4bfa326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{shift}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "# Finding a verb with a subject from below — good\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZZvgy44V11g",
    "outputId": "2b17caeb-ee86-4146-da87-84c9af476f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shift]\n"
     ]
    }
   ],
   "source": [
    "# Finding a verb with a subject from above — less good\n",
    "verbs = []\n",
    "for possible_verb in doc:\n",
    "    if possible_verb.pos == VERB:\n",
    "        for possible_subject in possible_verb.children:\n",
    "            if possible_subject.dep == nsubj:\n",
    "                verbs.append(possible_verb)\n",
    "                break\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19vnjVfkV11h",
    "outputId": "dae01458-ce5f-4c96-ba84-61dbb3597457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bright', 'red']\n",
      "['on']\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"bright red apples on the tree\")\n",
    "print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
    "print([token.text for token in doc[2].rights])  # ['on']\n",
    "print(doc[2].n_lefts)  # 2\n",
    "print(doc[2].n_rights)  # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHeqep_xV11h"
   },
   "source": [
    "# Named Entity Recognition  \n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title.     \n",
    "\n",
    "spaCy recognizes named entities in a document by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.    \n",
    "\n",
    "spaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default model identifies a variety of named and numeric entities, including companies, locations, organizations and products. You can add arbitrary classes to the entity recognition system, and update the model with new examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Br_jVp15V11h",
    "outputId": "98286948-d6f8-4553-b6d8-5bd5804065ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n",
      "John 60 64 PERSON\n",
      "Peter 66 71 PERSON\n",
      "Harvard University 77 95 ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion from John, Peter from Harvard University\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFLyu1urV11h"
   },
   "source": [
    "`Text: The original entity text.     \n",
    "Start: Index of start of entity in the Doc.   \n",
    "End: Index of end of entity in the Doc.    \n",
    "LabeL: Entity label, i.e. type. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X5oM8JzV11h"
   },
   "source": [
    "`TEXT\tSTART\tEND\tLABEL\tDESCRIPTION\n",
    "Apple\t0\t5\tORG\tCompanies, agencies, institutions.\n",
    "U.K.\t27\t31\tGPE\tGeopolitical entity, i.e. countries, cities, states.\n",
    "$1 billion\t44\t54\tMONEY\tMonetary values, including unit.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csAEz4R_V11h",
    "outputId": "3ec01b6c-59e5-47ae-84d0-3dacb2163a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ7QE-g5V11h"
   },
   "source": [
    "# Sentence Segmentation\n",
    "A Doc object’s sentences are available via the `Doc.sents` property.\n",
    "\n",
    "\n",
    "spaCy uses the dependency parser to determine sentence boundaries.\n",
    "**This is usually more accurate than a rule-based approach, but it also means you’ll need a statistical model and accurate predictions.** If your texts are closer to general-purpose news or web text, this should work well out-of-the-box. For social media or conversational text that doesn’t follow the same rules, your application may benefit from a custom rule-based implementation. You can either use the built-in Sentencizer or plug an entirely custom rule-based function into your processing pipeline.\n",
    "\n",
    "spaCy’s dependency parser respects already set boundaries, so you can preprocess your Doc using custom rules before it’s parsed. Depending on your text, this may also improve accuracy, since the parser is constrained to predict parses consistent with the sentence boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwX31I2wV11h"
   },
   "source": [
    "To view a Doc’s sentences, you can iterate over the `Doc.sents`, a generator that yields Span objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXjITuVMV11h",
    "outputId": "34633228-e582-438e-8d77-37c0d116a992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3w6DTsIV11i"
   },
   "source": [
    "# Rule-based pipeline component\n",
    "The Sentencizer component is a pipeline component that splits sentences on punctuation like ., ! or ?. You can plug it into your pipeline if you only need sentence boundaries without the dependency parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijEToclRV11i"
   },
   "source": [
    "# Word Vectors and Semantic Similarity\n",
    "**TRAINING WORD VECTORS**    \n",
    "Dense, real valued vectors representing distributional similarity information are now a cornerstone of practical NLP. The most common way to train these vectors is the Word2vec methods. If you need to train a word2vec model we recommend the implementation using Gensim.  \n",
    "\n",
    "Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec.  A few examples:\n",
    "\n",
    "array([2.0228000e-01, -7.66180009e-02, 3.70219992e-01, ... dtype=float32)  \n",
    "Data in text8:   \n",
    "71291 300  \n",
    "santamaria -0.328541 0.143057 0.200979 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVBKvI9FV11i"
   },
   "source": [
    "NOTE:  \n",
    "**spaCy’s** small models (packages that end in sm) don’t ship with word vectors for compactness purposes. They only include context-sensitive tensors. This means you can still use the `similarity()` methods to compare documents, spans and tokens – but the result won’t be as good, and individual tokens won’t have any vectors assigned. So in order to use real word vectors, you need to download a larger model:  `en_vectors_web_lg` or `en_vectors_web_lg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYOjPF1NV11i",
    "outputId": "2e4d8d2d-82fa-4964-e07b-ad24fdf427aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.43172   , -1.0467288 , -0.780927  ,  0.45387337,  0.22313687,\n",
       "       -0.4822587 ,  0.7796049 ,  1.9291902 ,  0.5263105 , -0.8860629 ,\n",
       "        0.31216276, -0.8671374 , -0.97779465,  0.62443805, -0.24205405,\n",
       "        0.70003366, -1.1135058 , -1.335113  ,  1.551111  , -0.27649075,\n",
       "       -0.49291223,  0.21266821, -1.2886872 , -0.4015749 ,  1.4845917 ,\n",
       "        0.26011398,  0.244566  ,  1.1891397 , -0.4271981 ,  0.32347038,\n",
       "       -0.2853762 ,  0.9585209 ,  1.3764503 ,  0.02244225, -0.4406012 ,\n",
       "       -2.0229402 ,  0.14719443,  1.6322699 ,  0.8777428 , -1.1011673 ,\n",
       "       -0.5580491 ,  0.16557175, -0.49441516,  0.7951485 , -0.7971488 ,\n",
       "       -0.04855184, -0.6682936 ,  0.65149677,  0.28960374,  0.02061486,\n",
       "        0.97740465,  0.2379916 ,  0.72662365, -0.71118784,  0.10751665,\n",
       "       -0.0982407 ,  0.66061974, -0.23507589,  0.304424  , -0.65600216,\n",
       "       -1.2755834 , -0.41828483,  1.0467389 , -0.20455568,  0.1618619 ,\n",
       "        0.6822717 , -0.06672494, -0.16852754,  0.12145358, -0.42135045,\n",
       "       -0.13758652,  1.6141541 ,  0.9650748 ,  0.63999003, -0.14702561,\n",
       "        0.24284407,  0.14128391, -0.6678508 , -0.32493967, -1.2624674 ,\n",
       "        0.2822321 , -0.96413857, -0.6104078 , -0.4091128 , -0.18481803,\n",
       "        1.1050515 , -0.55165374,  0.17582801, -1.0846602 ,  0.35955024,\n",
       "       -1.5655494 , -0.54753345,  2.4091485 ,  0.516353  ,  0.55651164,\n",
       "        1.1028976 ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "banana = nlp(u\"banana\")\n",
    "banana.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZWCV1nxV11i"
   },
   "source": [
    "Dense, real valued vectors representing distributional similarity information are now a cornerstone of practical NLP. The most common way to train these vectors is the Word2vec family of algorithms. If you need to train a word2vec model, we recommend Gensim, a library implemented in the Python.\n",
    "\n",
    "Models that come with built-in word vectors make them available as the `Token.vector` attribute. `Doc.vector` and `Span.vector` will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6CxbGoVV11i",
    "outputId": "49969f8b-3a4f-4a10-969f-2c53df087d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 6.814786 True\n",
      "cat True 7.3709016 True\n",
      "banana True 7.6460695 True\n",
      "afskfsd True 7.192256 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokens = nlp(u'dog cat banana afskfsd')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWVXVSh8V11i"
   },
   "source": [
    "`Text: The original token text.  \n",
    "has vector: Does the token have a vector representation?  \n",
    "Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values squared)`  \n",
    "`OOV: Out-of-vocabulary`\n",
    "\n",
    "The words “dog”, “cat” and “banana” are all pretty common in English, so they’re part of the model’s vocabulary, and come with a vector. The word “afskfsd” on the other hand is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of 0, which means it’s practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger models or loading in a full vector package. Example: en_vectors_web_lg, which includes over 1 million unique vectors.  \n",
    "  \n",
    "spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one.  \n",
    "\n",
    "Use:  \n",
    "**.similarity()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mX2-31wzV11j",
    "outputId": "9553a6c4-4177-4f90-8434-1916b0512b7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.8220816850662231\n",
      "dog banana 0.20909050107002258\n",
      "dog ship 0.07517027109861374\n",
      "cat dog 0.8220816850662231\n",
      "cat cat 1.0\n",
      "cat banana 0.2235882580280304\n",
      "cat ship 0.040839508175849915\n",
      "banana dog 0.20909050107002258\n",
      "banana cat 0.2235882580280304\n",
      "banana banana 1.0\n",
      "banana ship 0.05500999465584755\n",
      "ship dog 0.07517027109861374\n",
      "ship cat 0.040839508175849915\n",
      "ship banana 0.05500999465584755\n",
      "ship ship 1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')  # make sure to use larger model!\n",
    "tokens = nlp(u'dog cat banana ship')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HWU4CY9V11j"
   },
   "source": [
    "In this case, the model’s predictions are pretty on point. A dog is very similar to a cat, whereas a banana is not very similar to either of them. Identical tokens are obviously 100% similar to each other (just not always exactly 1.0, because of vector math and floating point imprecisions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "niPGJbphV11j"
   },
   "outputs": [],
   "source": [
    "# !pip install notebook --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SalBKJhYV11j"
   },
   "source": [
    "### Importing Embedded Vector Models\n",
    "Models with names ending in _lg (large) contain severla hundreds or milions of 300 dimensional embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhMCsmjkV11j",
    "outputId": "84c778ba-087b-4827-f0de-d330842e508b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "banana True 31.620354 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJFf3wSMV11j"
   },
   "source": [
    "`is_oov` identifies words not in the vocabulary, i.e out of vocabulary (oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzgW4jAjV11j"
   },
   "source": [
    "#### Similarity method\n",
    "Similarity method produces `cosine()` distance (similarity) betwen tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rx4fMSIoV11j",
    "outputId": "719ff12b-f4ff-4966-d479-139da0898dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.8220816850662231\n",
      "dog banana 0.20909050107002258\n",
      "cat dog 0.8220816850662231\n",
      "cat cat 1.0\n",
      "cat banana 0.2235882580280304\n",
      "banana dog 0.20909050107002258\n",
      "banana cat 0.2235882580280304\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\n",
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGG7QxkDV11j"
   },
   "source": [
    "### Importing Embedded Vector Models\n",
    "Models with names ending in _lg (large) contain severla hundreds or milions of 300 dimensional embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZEX_kcMV11j",
    "outputId": "07d6251f-69bb-46a5-f264-3731f030a348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "banana True 31.620354 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uSachgYV11k"
   },
   "source": [
    "`is_oov` identifies words not in the vocabulary, i.e out of vocabulary (oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocYMA55DV11k"
   },
   "source": [
    "#### Similarity method\n",
    "Similarity method produces cosine() distance (similarity) betwen tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSkS6D3QV11k",
    "outputId": "0f6dab57-4fa0-4a14-baa2-d30979a9612e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.8220816850662231\n",
      "dog banana 0.20909050107002258\n",
      "cat dog 0.8220816850662231\n",
      "cat cat 1.0\n",
      "cat banana 0.2235882580280304\n",
      "banana dog 0.20909050107002258\n",
      "banana cat 0.2235882580280304\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\n",
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "mF7Jb7-jV11k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_model(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    with open(File,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    print(f\"{len(glove_model)} words loaded!\")\n",
    "    return glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGxchUMyV11k",
    "outputId": "4a6269d8-d851-4fb1-cbce-7e0a23d67909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000 words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove_model('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNQIBL5bV11k",
    "outputId": "18f71c40-ad39-452a-ed2f-696f05d9e43f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zmQ2xjeV11k",
    "outputId": "2264d4e2-f6f7-4f65-c68f-196f68783324"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.get('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "LKj2Kq1hV11k"
   },
   "outputs": [],
   "source": [
    "# You can load Gloveimport pandas as pd\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "words = pd.read_table('glove.6B.100d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "yAPS-sX9V11k"
   },
   "outputs": [],
   "source": [
    "# Then to get the vector for a word:\n",
    "\n",
    "def vec(w):\n",
    "  return words.loc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuQLMuTWV11l",
    "outputId": "5db21881-1106-4728-a51d-f61da477845a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     -0.038194\n",
       "2     -0.244870\n",
       "3      0.728120\n",
       "4     -0.399610\n",
       "5      0.083172\n",
       "         ...   \n",
       "96    -0.510580\n",
       "97    -0.520280\n",
       "98    -0.145900\n",
       "99     0.827800\n",
       "100    0.270620\n",
       "Name: the, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Pf_Xz9WeV11l"
   },
   "outputs": [],
   "source": [
    "word_matrix = words.to_numpy()\n",
    "\n",
    "# And to find the closest word to a vector:\n",
    "def find_closest_word(v):\n",
    "  diff = words_matrix - v\n",
    "  delta = np.sum(diff * diff, axis=1)\n",
    "  i = np.argmin(delta)\n",
    "  return words.iloc[i].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLM7dcSuWxsX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
